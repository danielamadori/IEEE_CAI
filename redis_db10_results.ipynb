{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redis DB10 Results Explorer\n",
    "\n",
    "This notebook loads the Redis backup for database 10 stored in the results archives and decodes the serialized JSON payloads so they can be analysed as regular pandas tables.\n"
   ],
   "id": "821d0d415b5cd28c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "1. Update `BACKUP_SOURCE` below if you want to inspect a different archive or a raw JSON file.\n",
    "2. Run the notebook top-to-bottom to load the backup and build a DataFrame with decoded entries.\n",
    "3. Use the provided summaries or extend the notebook with your own analysis steps.\n"
   ],
   "id": "6d4fab348f3eba59"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:39:45.617325Z",
     "start_time": "2025-10-27T15:39:45.055748Z"
    }
   },
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import base64\n",
    "import binascii\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n"
   ],
   "id": "e89e74e31210798",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:39:45.644977Z",
     "start_time": "2025-10-27T15:39:45.627352Z"
    }
   },
   "source": [
    "class DumpDecodeError(RuntimeError):\n",
    "    'Raised when a Redis DUMP payload cannot be decoded.'\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DumpSections:\n",
    "    payload: bytes\n",
    "    version: int\n",
    "    checksum: bytes\n",
    "\n",
    "\n",
    "class _LengthEncoding:\n",
    "    __slots__ = ('value', 'encoding')\n",
    "\n",
    "    def __init__(self, value=None, encoding=None):\n",
    "        self.value = value\n",
    "        self.encoding = encoding\n",
    "\n",
    "\n",
    "RDB_ENCODING_INT8 = 0\n",
    "RDB_ENCODING_INT16 = 1\n",
    "RDB_ENCODING_INT32 = 2\n",
    "RDB_ENCODING_LZF = 3\n",
    "\n",
    "\n",
    "def split_dump_sections(raw: bytes) -> DumpSections:\n",
    "    if len(raw) < 10:\n",
    "        raise DumpDecodeError('DUMP payload is too short to contain metadata')\n",
    "    checksum = raw[-8:]\n",
    "    version_bytes = raw[-10:-8]\n",
    "    version = int.from_bytes(version_bytes, 'little', signed=False)\n",
    "    payload = raw[:-10]\n",
    "    return DumpSections(payload=payload, version=version, checksum=checksum)\n",
    "\n",
    "\n",
    "def _read_length_info(buffer: bytes, offset: int):\n",
    "    if offset >= len(buffer):\n",
    "        raise DumpDecodeError('Offset out of range while reading length')\n",
    "    first = buffer[offset]\n",
    "    prefix = first >> 6\n",
    "    if prefix == 0:\n",
    "        length = first & 0x3F\n",
    "        return _LengthEncoding(length), offset + 1\n",
    "    if prefix == 1:\n",
    "        if offset + 1 >= len(buffer):\n",
    "            raise DumpDecodeError('Truncated 14-bit encoded length')\n",
    "        second = buffer[offset + 1]\n",
    "        length = ((first & 0x3F) << 8) | second\n",
    "        return _LengthEncoding(length), offset + 2\n",
    "    if prefix == 2:\n",
    "        if offset + 4 >= len(buffer):\n",
    "            raise DumpDecodeError('Truncated 32-bit encoded length')\n",
    "        length = int.from_bytes(buffer[offset + 1 : offset + 5], 'big', signed=False)\n",
    "        return _LengthEncoding(length), offset + 5\n",
    "    return _LengthEncoding(None, first & 0x3F), offset + 1\n",
    "\n",
    "\n",
    "def lzf_decompress(data: bytes, expected_length: int) -> bytes:\n",
    "    output = bytearray()\n",
    "    idx = 0\n",
    "    data_len = len(data)\n",
    "    while idx < data_len:\n",
    "        ctrl = data[idx]\n",
    "        idx += 1\n",
    "        if ctrl < 32:\n",
    "            literal_len = ctrl + 1\n",
    "            if idx + literal_len > data_len:\n",
    "                raise DumpDecodeError('Truncated literal LZF sequence')\n",
    "            output.extend(data[idx : idx + literal_len])\n",
    "            idx += literal_len\n",
    "        else:\n",
    "            length = ctrl >> 5\n",
    "            ref_offset = len(output) - ((ctrl & 0x1F) << 8) - 1\n",
    "            if length == 7:\n",
    "                if idx >= data_len:\n",
    "                    raise DumpDecodeError('Truncated LZF sequence while extending length')\n",
    "                length += data[idx]\n",
    "                idx += 1\n",
    "            if idx >= data_len:\n",
    "                raise DumpDecodeError('Truncated LZF sequence while resolving reference')\n",
    "            ref_offset -= data[idx]\n",
    "            idx += 1\n",
    "            length += 2\n",
    "            if ref_offset < 0:\n",
    "                raise DumpDecodeError('Negative LZF reference')\n",
    "            for _ in range(length):\n",
    "                if ref_offset >= len(output):\n",
    "                    raise DumpDecodeError('LZF reference out of range')\n",
    "                output.append(output[ref_offset])\n",
    "                ref_offset += 1\n",
    "    if len(output) != expected_length:\n",
    "        raise DumpDecodeError(\n",
    "            f\"Unexpected decompressed length: expected {expected_length}, got {len(output)}\"\n",
    "        )\n",
    "    return bytes(output)\n",
    "\n",
    "\n",
    "def _decode_special_encoding(buffer: bytes, offset: int, encoding: int):\n",
    "    if encoding == RDB_ENCODING_INT8:\n",
    "        if offset >= len(buffer):\n",
    "            raise DumpDecodeError('Truncated 8-bit encoded integer')\n",
    "        value = int.from_bytes(buffer[offset : offset + 1], 'little', signed=True)\n",
    "        return str(value).encode('ascii'), offset + 1\n",
    "    if encoding == RDB_ENCODING_INT16:\n",
    "        if offset + 2 > len(buffer):\n",
    "            raise DumpDecodeError('Truncated 16-bit encoded integer')\n",
    "        value = int.from_bytes(buffer[offset : offset + 2], 'little', signed=True)\n",
    "        return str(value).encode('ascii'), offset + 2\n",
    "    if encoding == RDB_ENCODING_INT32:\n",
    "        if offset + 4 > len(buffer):\n",
    "            raise DumpDecodeError('Truncated 32-bit encoded integer')\n",
    "        value = int.from_bytes(buffer[offset : offset + 4], 'little', signed=True)\n",
    "        return str(value).encode('ascii'), offset + 4\n",
    "    if encoding == RDB_ENCODING_LZF:\n",
    "        compressed_len_info, next_offset = _read_length_info(buffer, offset)\n",
    "        data_len_info, data_offset = _read_length_info(buffer, next_offset)\n",
    "        if compressed_len_info.value is None or data_len_info.value is None:\n",
    "            raise DumpDecodeError('Invalid LZF length encoding')\n",
    "        end = data_offset + compressed_len_info.value\n",
    "        if end > len(buffer):\n",
    "            raise DumpDecodeError('Truncated encoded string')\n",
    "        compressed = buffer[data_offset:end]\n",
    "        decompressed = lzf_decompress(compressed, data_len_info.value)\n",
    "        return decompressed, end\n",
    "    raise DumpDecodeError('Unknown string encoding')\n",
    "\n",
    "\n",
    "def _read_encoded_string(buffer: bytes, offset: int):\n",
    "    length_info, next_offset = _read_length_info(buffer, offset)\n",
    "    if length_info.encoding is None:\n",
    "        end = next_offset + length_info.value\n",
    "        if end > len(buffer):\n",
    "            raise DumpDecodeError('Truncated encoded string')\n",
    "        return buffer[next_offset:end], end\n",
    "    return _decode_special_encoding(buffer, next_offset, length_info.encoding)\n",
    "\n",
    "\n",
    "def decode_string_from_dump(raw: bytes) -> bytes:\n",
    "    sections = split_dump_sections(raw)\n",
    "    payload = sections.payload\n",
    "    if not payload:\n",
    "        raise DumpDecodeError('Empty payload')\n",
    "    object_type = payload[0]\n",
    "    if object_type != 0:\n",
    "        raise DumpDecodeError(f'Non-string object type: {object_type}')\n",
    "    value, _ = _read_encoded_string(payload, 1)\n",
    "    return value\n",
    "\n",
    "\n",
    "def decode_base64_bytes(value: str) -> bytes:\n",
    "    if not isinstance(value, str):\n",
    "        raise DumpDecodeError('Encoded value must be a base64 string')\n",
    "    try:\n",
    "        return base64.b64decode(value.encode('ascii'))\n",
    "    except (UnicodeEncodeError, binascii.Error) as exc:\n",
    "        raise DumpDecodeError(f'Invalid base64 payload: {exc}') from exc\n",
    "\n",
    "\n",
    "def decode_entry(entry: dict) -> dict:\n",
    "    key_bytes = decode_base64_bytes(entry['key'])\n",
    "    key_text = key_bytes.decode('utf-8', errors='replace')\n",
    "    value_info = entry.get('value') or {}\n",
    "    data_b64 = value_info.get('data')\n",
    "    if not data_b64:\n",
    "        raise DumpDecodeError('Missing DUMP payload in backup entry')\n",
    "    raw_value = decode_base64_bytes(data_b64)\n",
    "    decoded_bytes = decode_string_from_dump(raw_value)\n",
    "    text_value = decoded_bytes.decode('utf-8', errors='replace')\n",
    "    try:\n",
    "        json_value = json.loads(text_value)\n",
    "    except json.JSONDecodeError:\n",
    "        json_value = None\n",
    "    return {\n",
    "        'redis_key': key_text,\n",
    "        'decoded_bytes': decoded_bytes,\n",
    "        'text': text_value,\n",
    "        'json': json_value,\n",
    "    }\n"
   ],
   "id": "6e13f0eb37817ebc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:39:45.660446Z",
     "start_time": "2025-10-27T15:39:45.654707Z"
    }
   },
   "source": [
    "def load_backup_payload(source: Path, db_number: int = 10) -> dict:\n",
    "    'Load a redis_backup_dbXX.json payload from a JSON file, directory, or ZIP archive.'\n",
    "    source = source.expanduser()\n",
    "    if source.is_dir():\n",
    "        candidate = source / f'redis_backup_db{db_number}.json'\n",
    "        if not candidate.exists():\n",
    "            raise FileNotFoundError(f'Backup file not found under {source}')\n",
    "        return json.loads(candidate.read_text(encoding='utf-8'))\n",
    "    if not source.exists():\n",
    "        raise FileNotFoundError(f'Backup source not found: {source}')\n",
    "    suffix = source.suffix.lower()\n",
    "    if suffix == '.json':\n",
    "        return json.loads(source.read_text(encoding='utf-8'))\n",
    "    if suffix == '.zip':\n",
    "        target_name = f'redis_backup_db{db_number}.json'\n",
    "        with ZipFile(source) as archive:\n",
    "            matches = [name for name in archive.namelist() if name.endswith(target_name)]\n",
    "            if not matches:\n",
    "                raise FileNotFoundError(f'{target_name} not found inside {source.name}')\n",
    "            if len(matches) > 1:\n",
    "                print(f'Warning: multiple matches found, using {matches[0]}')\n",
    "            data = archive.read(matches[0])\n",
    "            return json.loads(data.decode('utf-8'))\n",
    "    raise ValueError(f'Unsupported backup source: {source}')\n"
   ],
   "id": "f4b6e5e241c9c8af",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:39:45.685774Z",
     "start_time": "2025-10-27T15:39:45.676262Z"
    }
   },
   "source": [
    "results_dir = Path('results')\n",
    "available_archives = sorted(results_dir.glob('*.zip'))\n",
    "available_archives\n"
   ],
   "id": "df9974375b1d7bb5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('results/ECG200_-1_false_0.zip'),\n",
       " WindowsPath('results/HandOutlines_0_false_0.zip'),\n",
       " WindowsPath('results/MelbournePedestrian_1_false_0.zip'),\n",
       " WindowsPath('results/MiddlePhalanxOutlineCorrect_0_false_0.zip'),\n",
       " WindowsPath('results/SonyAIBORobotSurface1_1_false_0.zip'),\n",
       " WindowsPath('results/Wafer_-1_false_0.zip'),\n",
       " WindowsPath('results/Wine_1_false_0.zip')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:39:46.056421Z",
     "start_time": "2025-10-27T15:39:45.706382Z"
    }
   },
   "source": [
    "BACKUP_SOURCE = Path('results') / 'SonyAIBORobotSurface1_1_false_0.zip'\n",
    "DB_NUMBER = 10\n",
    "\n",
    "backup_payload = load_backup_payload(BACKUP_SOURCE, db_number=DB_NUMBER)\n",
    "entries = backup_payload.get('entries', [])\n",
    "\n",
    "print(f'Loaded {len(entries)} entries from {BACKUP_SOURCE}')\n",
    "metadata = backup_payload.get('metadata')\n",
    "if metadata:\n",
    "    print('Metadata:')\n",
    "    print(json.dumps(metadata, indent=2))\n"
   ],
   "id": "8bcf83f723e645f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 32738 entries from results\\SonyAIBORobotSurface1_1_false_0.zip\n",
      "Metadata:\n",
      "{\n",
      "  \"created_at_utc\": \"2025-10-25T13:52:26.924139Z\",\n",
      "  \"key_count\": 32738,\n",
      "  \"scan_count\": 1000,\n",
      "  \"source\": {\n",
      "    \"db\": 10,\n",
      "    \"host\": \"127.0.0.1\",\n",
      "    \"port\": 6379\n",
      "  },\n",
      "  \"type_summary\": {\n",
      "    \"string\": 32738\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:39:53.510674Z",
     "start_time": "2025-10-27T15:39:46.079940Z"
    }
   },
   "source": [
    "decoded_entries = []\n",
    "failed_entries = []\n",
    "\n",
    "for entry in entries:\n",
    "    try:\n",
    "        decoded_entries.append(decode_entry(entry))\n",
    "    except DumpDecodeError as exc:\n",
    "        failed_entries.append({'entry': entry, 'error': str(exc)})\n",
    "\n",
    "print(f'Decoded {len(decoded_entries)} entries; {len(failed_entries)} failures.')\n",
    "if failed_entries:\n",
    "    print('First failure:')\n",
    "    print(failed_entries[0]['error'])\n"
   ],
   "id": "9c0b8ff09907f5bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded 32738 entries; 0 failures.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:39:56.536027Z",
     "start_time": "2025-10-27T15:39:56.528373Z"
    }
   },
   "source": [
    "if decoded_entries:\n",
    "    sample = decoded_entries[0]\n",
    "    print(f\"Sample key: {sample['redis_key']}\")\n",
    "    print(json.dumps(sample['json'], indent=2))\n"
   ],
   "id": "5f5a52936c911c49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample key: thething:worker_24003:14\n",
      "{\n",
      "  \"worker_id\": \"thething:worker_24003\",\n",
      "  \"iteration\": 14,\n",
      "  \"timestamp_start\": \"2025-10-25T13:54:48.637414\",\n",
      "  \"queue_size\": 54,\n",
      "  \"car_queue_size\": 1,\n",
      "  \"car_processing\": {\n",
      "    \"candidate_bitmap\": \"111011011110111011011011101101111101101100110111111111111110110110111011011\",\n",
      "    \"icf_size\": 24,\n",
      "    \"result\": \"CONFIRMED_AR\",\n",
      "    \"time_seconds\": 0.10586810111999512,\n",
      "    \"raw_info\": {\n",
      "      \"ar_iterations\": 1,\n",
      "      \"ar_profile_dominated_by_AP\": 1,\n",
      "      \"deleted_from_AR\": 1,\n",
      "      \"deleted_from_AP\": 2,\n",
      "      \"deleted_from_CAR\": 9,\n",
      "      \"ar_extensions_total\": 17,\n",
      "      \"ar_ext_AR_cache_checks\": 3638,\n",
      "      \"ar_ext_R_cache_checks\": 4040,\n",
      "      \"ar_ext_R_cache_hits\": 1,\n",
      "      \"ar_ext_R_shares_sample\": 1,\n",
      "      \"ar_extensions_filtered_by_R\": 1,\n",
      "      \"ar_extensions_added\": 16,\n",
      "      \"ar_extensions_added_to_CAR\": 16\n",
      "    },\n",
      "    \"extensions\": {\n",
      "      \"total\": 17,\n",
      "      \"added\": 0,\n",
      "      \"filtered\": 17,\n",
      "      \"filtered_by_ar\": 0,\n",
      "      \"filtered_by_r_sharing\": 1,\n",
      "      \"filtered_by_ap\": 16\n",
      "    }\n",
      "  },\n",
      "  \"can_processing\": {\n",
      "    \"candidate_bitmap\": \"110011111110111111110011101111111001111000111101101101100111100011011011110\",\n",
      "    \"icf_size\": 24,\n",
      "    \"result\": \"GOOD\",\n",
      "    \"time_seconds\": 0.002104520797729492,\n",
      "    \"raw_info\": {\n",
      "      \"iterations\": 1,\n",
      "      \"dominated_by_R\": 1,\n",
      "      \"deleted_from_R\": 0,\n",
      "      \"deleted_from_GP\": 0,\n",
      "      \"deleted_from_CAN\": 0\n",
      "    },\n",
      "    \"extensions\": {\n",
      "      \"total\": 20,\n",
      "      \"added\": 12,\n",
      "      \"filtered\": 8\n",
      "    }\n",
      "  },\n",
      "  \"outcomes\": {\n",
      "    \"car_confirmed_ar\": true,\n",
      "    \"good\": true\n",
      "  },\n",
      "  \"timings\": {\n",
      "    \"total_seconds\": 0.5415184497833252,\n",
      "    \"car_seconds\": 0.10586810111999512,\n",
      "    \"can_seconds\": 0.002104520797729492\n",
      "  },\n",
      "  \"timestamp_end\": \"2025-10-25T13:54:49.178940\"\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:41:30.434549Z",
     "start_time": "2025-10-27T15:39:56.543544Z"
    }
   },
   "source": [
    "records = []\n",
    "for decoded in decoded_entries:\n",
    "    payload = decoded['json']\n",
    "    if payload is None:\n",
    "        continue\n",
    "    flat = pd.json_normalize(payload, sep='_')\n",
    "    flat['redis_key'] = decoded['redis_key']\n",
    "    records.append(flat)\n",
    "\n",
    "if records:\n",
    "    df = pd.concat(records, ignore_index=True)\n",
    "    df = df.set_index('redis_key')\n",
    "    df.head()\n",
    "else:\n",
    "    df = pd.DataFrame()\n",
    "    df\n"
   ],
   "id": "dc8e672a7db8122a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:41:30.990357Z",
     "start_time": "2025-10-27T15:41:30.937380Z"
    }
   },
   "source": [
    "if not df.empty:\n",
    "    summary = {\n",
    "        'worker_count': int(df['worker_id'].nunique()),\n",
    "        'total_iterations': int(df['iteration'].max()),\n",
    "        'total_runtime_hours': float(df['timings_total_seconds'].sum() / 3600.0),\n",
    "    }\n",
    "    print('Summary:')\n",
    "    print(json.dumps(summary, indent=2))\n",
    "    print('CAR results:')\n",
    "    print(df['car_processing_result'].value_counts())\n",
    "    print('CAN results:')\n",
    "    print(df['can_processing_result'].value_counts())\n"
   ],
   "id": "160458ad9922d5ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "{\n",
      "  \"worker_count\": 32,\n",
      "  \"total_iterations\": 1066,\n",
      "  \"total_runtime_hours\": 61.61795484377278\n",
      "}\n",
      "CAR results:\n",
      "car_processing_result\n",
      "CONFIRMED_AR    29047\n",
      "NOT_AR           3687\n",
      "Name: count, dtype: int64\n",
      "CAN results:\n",
      "can_processing_result\n",
      "GOOD    13608\n",
      "BAD      3898\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:41:31.062030Z",
     "start_time": "2025-10-27T15:41:31.013803Z"
    }
   },
   "source": [
    "if not df.empty:\n",
    "    per_worker = (\n",
    "        df.groupby('worker_id')\n",
    "        .agg({\n",
    "            'iteration': ['count', 'max'],\n",
    "            'queue_size': ['mean', 'max'],\n",
    "            'timings_total_seconds': 'sum',\n",
    "        })\n",
    "    )\n",
    "    per_worker.columns = ['_'.join(map(str, col)).strip('_') for col in per_worker.columns]\n",
    "    per_worker.head()\n"
   ],
   "id": "74e26e6658e98462",
   "outputs": [],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
